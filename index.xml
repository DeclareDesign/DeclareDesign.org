<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Home Page on A Hugo website</title>
    <link>/</link>
    <description>Recent content in Home Page on A Hugo website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 08 Jan 2020 00:00:00 +0000</lastBuildDate><atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Now there is a web interface for declaring and diagnosing research designs</title>
      <link>/blog/now-there-is-a-web-interface-for-declaring-and-diagnosing-research-designs.html</link>
      <pubDate>Wed, 08 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/now-there-is-a-web-interface-for-declaring-and-diagnosing-research-designs.html</guid>
      <description>By Clara Bicalho, Sisi Huang, and Markus Konrad
DeclareDesign is a collection of tools to help you “declare” and “diagnose” research designs. In a word, with the DeclareDesign packages you can quickly state the core analysis-relevant features of a research design, and in return you will get a diagnosis that tells you how well your design is likely to perform and how changes in the design could improve performance.</description>
    </item>
    
    <item>
      <title>An instrument does not have to be exogenous to be consistent</title>
      <link>/blog/2019-02-05-instrumental-variables.html</link>
      <pubDate>Tue, 19 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019-02-05-instrumental-variables.html</guid>
      <description>We often think of an instrumental variable (\(Z\)) as a random shock that generates exogenous variation in a treatment of interest \(X\). The randomness of \(Z\) lets us identify the effect of \(X\) on \(Y\), at least for units for which \(Z\) perturbs \(X\) in a way that’s not possible by just looking at the relationship between \(X\) and \(Y\). But surprisingly, we think, if effects are constant the instrumental variables estimator can be consistent for the effect of \(X\) on \(Y\) even when the relationship between the instrument (\(Z\)) and the endogenous variable (\(X\)) is confounded (for example, Hernán and Robins (2006)).</description>
    </item>
    
    <item>
      <title>Some designs have badly posed questions and design diagnosis can alert you to the problem</title>
      <link>/blog/some-designs-have-badly-posed-questions-and-design-diagnosis-can-alert-you-to-the-problem.html</link>
      <pubDate>Tue, 12 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/some-designs-have-badly-posed-questions-and-design-diagnosis-can-alert-you-to-the-problem.html</guid>
      <description>An obvious requirement of a good research design is that the question it seeks to answer does in fact have an answer, at least under plausible models of the world. But we can sometimes get quite far along a research path without being conscious that the questions we ask do not have answers and the answers we get are answering different questions.
How could a question not have an answer?</description>
    </item>
    
    <item>
      <title>Estimating Average Treatment Effects with Ordered Probit: Is it worth it?</title>
      <link>/blog/estimating-average-treatment-effects-with-ordered-probit-is-it-worth-it.html</link>
      <pubDate>Wed, 06 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/estimating-average-treatment-effects-with-ordered-probit-is-it-worth-it.html</guid>
      <description>We sometimes worry about whether we need to model data generating processes correctly. For example you have ordinal outcome variables, on a five-point Likert scale. How should you model the data generation process? Do you need to model it at all? Go-to approaches include ordered probit and ordered logit models which are designed for this kind of outcome variable. But maybe you don’t need them. After all, the argument that the difference-in-means procedure estimates the treatment effect doesn’t depend on any assumptions about the type of data (as long as expectations are defined)—ordered, count, censored, etc.</description>
    </item>
    
    <item>
      <title>What can you learn from simulating qualitative inference strategies?</title>
      <link>/blog/2019-01-30-process-tracing.html</link>
      <pubDate>Wed, 30 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019-01-30-process-tracing.html</guid>
      <description>Qualitative process-tracing sometimes seeks to answer “cause of effects” claims using within-case data: how probable is the hypothesis that \(X\) did in fact cause \(Y\)? Fairfield and Charman (2017), for example, ask whether the right changed position on tax reform during the 2005 Chilean presidential election (\(Y\)) because of anti-inequality campaigns (\(X\)) by examining whether the case study narrative bears evidence that you would only expect to see if this were true.</description>
    </item>
    
    <item>
      <title>Should a pilot study change your study design decisions?</title>
      <link>/blog/2019-01-23-pilot-studies.html</link>
      <pubDate>Wed, 23 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019-01-23-pilot-studies.html</guid>
      <description>Data collection is expensive, and we often only get one bite at the apple. In response, we often conduct an inexpensive (and small) pilot test to help better design the study. Pilot studies have many virtues, including practicing the logistics of data collection and improving measurement tools. But using pilots to get noisy estimates in order to determine sample sizes for scale up comes with risks.
Pilot studies are often used to get a guess of the average effect size, which is then plugged into power calculators when designing the full study.</description>
    </item>
    
    <item>
      <title>Use change scores or control for pre-treatment outcomes? Depends on the true data generating process</title>
      <link>/blog/use-change-scores-or-control-for-pre-treatment-outcomes-depends-on-the-true-data-generating-process.html</link>
      <pubDate>Tue, 15 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/use-change-scores-or-control-for-pre-treatment-outcomes-depends-on-the-true-data-generating-process.html</guid>
      <description>We’re in an observational study setting in which treatment assignment was not controlled by the researcher. We have pre-treatment data on baseline outcomes and we’d like to incorporate them, mainly to decrease bias due to confounding and but also, ideally, to increase precision. One approach is to use the difference between pre and post outcomes as the outcome variable; another is to use the baseline data as a control.</description>
    </item>
    
    <item>
      <title>A journal of null results is a flawed fix for a significance filter</title>
      <link>/blog/a-journal-of-null-results-is-a-flawed-fix-for-a-significance-filter.html</link>
      <pubDate>Tue, 08 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/a-journal-of-null-results-is-a-flawed-fix-for-a-significance-filter.html</guid>
      <description>Mostly we use design diagnostics to assess issues that arise because of design decisions. But you can also use these tools to examine issues that arise after implementation. Here we look at risks from publication bias and illustrate two distinct types of upwards bias that arise from a “significance filter.” A journal for publishing null results might help, but the results in there are also likely to be biased, downwards.</description>
    </item>
    
    <item>
      <title>DeclareDesign Holiday Hiatus</title>
      <link>/blog/declaredesign-holiday-hiatus.html</link>
      <pubDate>Thu, 20 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/declaredesign-holiday-hiatus.html</guid>
      <description>  We’ll be back on January 7 – Happy New Year!
library(DeclareDesign) library(ggplot2) pop &amp;lt;- declare_population( N = 14, X = c(&amp;quot;H&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;P&amp;quot;, &amp;quot;P&amp;quot;, &amp;quot;Y&amp;quot;, &amp;quot; &amp;quot;, &amp;quot;H&amp;quot;, &amp;quot;O&amp;quot;, &amp;quot;L&amp;quot;, &amp;quot;I&amp;quot;, &amp;quot;D&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;Y&amp;quot;, &amp;quot;S&amp;quot;), position = N:1, index = match(X, LETTERS) ) ggplot(data = pop(), aes(index, position, color = X)) + geom_text(aes(label = X)) + theme_bw() + theme(legend.position = &amp;quot;none&amp;quot;, axis.title = element_blank()) </description>
    </item>
    
    <item>
      <title>Sometimes you need to cluster standard errors above the level of treatment</title>
      <link>/blog/sometimes-you-need-to-cluster-standard-errors-above-the-level-of-treatment.html</link>
      <pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/sometimes-you-need-to-cluster-standard-errors-above-the-level-of-treatment.html</guid>
      <description>In designs in which a treatment is assigned in clusters (e.g. classrooms), it’s usual practice to account for cluster-level correlations when you generate estimates of uncertainty about estimated effects. But units often share commonalities at higher levels, such as at a block level (e.g. schools). Sometimes you need to take account of this and sometimes you don’t. We show an instance of the usual procedure of clustering by assignment cluster (classrooms) working well and show how badly you can do with a more conservative approach (clustering by schools).</description>
    </item>
    
    <item>
      <title>Meta-analysis can be used not just to guess about effects out-of-sample but also to re-evaluate effects in sample</title>
      <link>/blog/meta-analysis-can-be-used-not-just-to-guess-about-effects-out-of-sample-but-also-to-re-evaluate-effects-in-sample.html</link>
      <pubDate>Tue, 11 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/meta-analysis-can-be-used-not-just-to-guess-about-effects-out-of-sample-but-also-to-re-evaluate-effects-in-sample.html</guid>
      <description>Imagine you are in the fortunate position of planning a collection of studies which you will later get to analyze together (looking at you metaketas). Each study estimates a site specific effect. You want to learn something about general effects. We work through design issues using a multi-study design with J studies that employs both frequentist and Bayesian approaches to meta-analysis. In the designs that we diagnose these perform very similarly in terms of estimating sample and population average effects.</description>
    </item>
    
    <item>
      <title>Get me a random assignment YESTERDAY</title>
      <link>/blog/get-me-a-random-assignment-yesterday.html</link>
      <pubDate>Tue, 04 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/get-me-a-random-assignment-yesterday.html</guid>
      <description>You’re partnering with an education nonprofit and you are planning on running a randomized control trial in 80 classrooms spread across 20 community schools. The request is in: please send us a spreadsheet with random assignments. The assignment’s gotta be blocked by school, it’s gotta be reproducible, and it’s gotta be tonight. The good news is that you can do all this in a couple of lines of code.</description>
    </item>
    
    <item>
      <title>Randomization does not justify t-tests. How worried should I be?</title>
      <link>/blog/randomization-does-not-justify-t-tests.-how-worried-should-i-be.html</link>
      <pubDate>Tue, 27 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/randomization-does-not-justify-t-tests.-how-worried-should-i-be.html</guid>
      <description>Deaton and Cartwright (2017) provide multiple arguments against claims that randomized trials should be thought of as a kind of gold standard of scientific evidence. One striking argument they make is that randomization does not justify the statistical tests that researchers typically use. They are right in that. Even if researchers can claim that their estimates of uncertainty are justified by randomization, their habitual use of those estimates to conduct t-tests are not.</description>
    </item>
    
    <item>
      <title>Instead of avoiding spillovers, you can model them</title>
      <link>/blog/modelling_spillovers.html</link>
      <pubDate>Tue, 20 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/modelling_spillovers.html</guid>
      <description>Spillovers are often seen as a nuisance that lead researchers into error when estimating effects of interest. In a previous post, we discussed sampling strategies to reduce these risks. A more substantively satisfying approach is to try to study spillovers directly. If we do it right we can remove errors in our estimation of primary quantities of interest and learn about how spillovers work at the same time.</description>
    </item>
    
    <item>
      <title>What does a p-value tell you about the probability a hypothesis is true?</title>
      <link>/blog/2018-11-13-learning-from-p.html</link>
      <pubDate>Tue, 13 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-11-13-learning-from-p.html</guid>
      <description>The humble \(p\)-value is much maligned and terribly misunderstood. The problem is that everyone wants to know the answer to the question: “what is the probability that [hypothesis] is true?” But \(p\) answers a different (and not terribly useful) question: “how (un)surprising is this evidence given [hypothesis]?” Can \(p\) shed insight on the question we really care about? Maybe, though there are dangers.
This post is inspired by conversations with @david_colquhoun who has been doing a lot of work on the misinterpretation of p-values (see especially “The false positive risk: a proposal concerning what to do about p values”).</description>
    </item>
    
    <item>
      <title>Common estimators of uncertainty overestimate uncertainty</title>
      <link>/blog/neyman-sate-pate.html</link>
      <pubDate>Wed, 07 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/neyman-sate-pate.html</guid>
      <description>Random assignment provides a justification not just for estimates of effects but also for estimates of uncertainty about effects. The basic approach, due to Neyman, is to estimate the variance in estimates of the difference between outcomes in treatment and in control outcomes using the variability that can be observed among units in control and units in treatment. It’s an ingenious approach and dispenses with the need to make any assumptions about the shape of statistical distributions or about asymptotics.</description>
    </item>
    
    <item>
      <title>Cluster randomized trials can be biased when cluster sizes are heterogeneous</title>
      <link>/blog/bias-cluster-randomized-trials.html</link>
      <pubDate>Wed, 31 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/bias-cluster-randomized-trials.html</guid>
      <description>In many experiments, random assignment is performed at the level of clusters. Researchers are conscious that in such cases they cannot rely on the usual standard errors and they should take account of this feature by clustering their standard errors. Another, more subtle, risk in such designs is that if clusters are of different sizes, clustering can actually introduce bias, even if all clusters are assigned to treatment with the same probability.</description>
    </item>
    
    <item>
      <title>With great power comes great responsibility</title>
      <link>/blog/with-great-power-comes-great-responsibility.html</link>
      <pubDate>Tue, 23 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/with-great-power-comes-great-responsibility.html</guid>
      <description>We usually think that the bigger the study the better. And so huge studies often rightly garner great publicity. But the ability to generate more precise results also comes with a risk. If study designs are at risk of bias and readers (or publicists!) employ a statistical significance filter, then big data might not remove threats of bias and might actually make things worse.
Motivating Example: Exercise and Mental Health A recent study in the Lancet by Chekroud et al.</description>
    </item>
    
    <item>
      <title>How misleading are clustered SEs in designs with few clusters?</title>
      <link>/blog/how-misleading-are-clustered-ses-in-designs-with-few-clusters.html</link>
      <pubDate>Tue, 16 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/how-misleading-are-clustered-ses-in-designs-with-few-clusters.html</guid>
      <description>Cluster-robust standard errors are known to behave badly with too few clusters. There is a great discussion of this issue by Berk Özler “Beware of studies with a small number of clusters” drawing on studies by Cameron, Gelbach, and Miller (2008). See also this nice post by Cyrus Samii and a recent treatment by Esarey and Menger (2018). A rule of thumb is to start worrying about sandwich estimators when the number of clusters goes below 40.</description>
    </item>
    
    <item>
      <title>The trouble with &#39;controlling for blocks&#39;</title>
      <link>/blog/biased-fixed-effects.html</link>
      <pubDate>Tue, 09 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/biased-fixed-effects.html</guid>
      <description>In many experiments, different groups of units get assigned to treatment with different probabilities. This can give rise to misleading results unless you properly take account of possible differences between the groups. How best to do this? The go-to approach is to “control” for groups by introducing “fixed-effects” in a regression set-up. The bad news is that this procedure is prone to bias. The good news is that there’s an even simpler and more intuitive approach that gets it right: estimate the difference-in-means within each group, then average over these group-level estimates weighting according to the size of the group.</description>
    </item>
    
    <item>
      <title>Improve power using your answer strategy, not just your data strategy</title>
      <link>/blog/improve-power-using-your-answer-strategy-not-just-your-data-strategy.html</link>
      <pubDate>Tue, 02 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/improve-power-using-your-answer-strategy-not-just-your-data-strategy.html</guid>
      <description>Most power calculators take a small number of inputs: sample size, effect size, and variance. Some also allow for number of blocks or cluster size as well as the overall sample size. All of these inputs relate to your data strategy. Unless you can control the effect size and the noise, you are left with sample size and data structure (blocks and clusters) as the only levers to play with to try to improve your power.</description>
    </item>
    
    <item>
      <title>Sometimes blocking can reduce your precision</title>
      <link>/blog/sometimes-blocking-can-reduce-your-precision.html</link>
      <pubDate>Mon, 24 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/sometimes-blocking-can-reduce-your-precision.html</guid>
      <description>You can often improve the precision of your randomized controlled trial with blocking: first gather similar units together into groups, then run experiments inside each little group, then average results across experiments. Block random assignment (sometimes called stratified random assignment) can be great—increasing precision with blocking is like getting extra sample size for free. Blocking works because it’s like controlling for a pre-treatment covariate in the “Data Strategy” rather than in the “Answer Strategy.</description>
    </item>
    
    <item>
      <title>You can&#39;t speak meaningfully about spillovers without specifying an estimand</title>
      <link>/blog/you-cant-speak-meaningfully-about-spillovers-without-specifying-an-estimand.html</link>
      <pubDate>Tue, 18 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/you-cant-speak-meaningfully-about-spillovers-without-specifying-an-estimand.html</guid>
      <description>A dangerous fact: it is quite possible to talk in a seemingly coherent way about strategies to answer a research question without ever properly specifying what the research question is. The risk is that you end up with the right solution to the wrong problem. The problem is particularly acute for studies where there are risks of “spillovers.”
By spillovers we mean situations where one unit’s outcome depends upon how another unit is assigned to treatment.</description>
    </item>
    
    <item>
      <title>How controlling for pretreatment covariates can introduce bias</title>
      <link>/blog/how-controlling-for-pretreatment-covariates-can-introduce-bias.html</link>
      <pubDate>Wed, 12 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/how-controlling-for-pretreatment-covariates-can-introduce-bias.html</guid>
      <description>Consider an observational study looking at the effect of a non-randomly assigned treatment, \(Z\), on an outcome \(Y\). Say you have a pretreatment covariate, \(X\), that is correlated with both \(Z\) and \(Y\). Should you control for \(X\) when you try to assess the effect of \(Z\) on \(Y\)?
This has been a question of some disagreement, with Rosenbaum (2002), for instance, arguing that “there is little to no reason to avoid adjustment for a true covariate, a variable describing subjects before treatment,” and Greenland, Pearl, and Robins (1999) and others arguing that you cannot answer this question without a causal model of how \(Z\) relates to \(X\) and \(Y\).</description>
    </item>
    
    <item>
      <title>DeclareDesign: The Blog</title>
      <link>/blog/declaredesign-blog.html</link>
      <pubDate>Tue, 11 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/declaredesign-blog.html</guid>
      <description>Welcome to the DeclareDesign blog! We have been working on developing the DeclareDesign family of software packages to let researchers easily generate research designs and assess their properties. Our plan over the next six months is to put up weekly blog posts showing off features of the packages or highlighting the kinds of things you can learn about research design using this approach.
Our very first blog post is not here but over at the World Bank Development Impact blog.</description>
    </item>
    
    <item>
      <title>About DeclareDesign</title>
      <link>/about.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/about.html</guid>
      <description>The DeclareDesign project is directed by Graeme Blair (UCLA), Jasper Cooper (Columbia University), Alexander Coppock (Yale University), and Macartan Humphreys (Columbia University and WZB).
Project contributors DeclareDesign has benefited from the contributions of many researchers and coders. We recognize key contributors in addition to the principal investigators to each component:
DeclareDesign for R: Neal Fultz
estimatr for R: Luke Sonnet
fabricatr for R: Aaron Rudkin
randomizr for Stata: John Ternovski</description>
    </item>
    
    <item>
      <title>Contributor Covenant Code of Conduct</title>
      <link>/conduct.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/conduct.html</guid>
      <description>Our Pledge In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.
 Our Standards Examples of behavior that contributes to creating a positive environment include:</description>
    </item>
    
    <item>
      <title>Getting Started with DeclareDesign</title>
      <link>/getting-started.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/getting-started.html</guid>
      <description>Installing R Where we are going Building a step of a research design Declaring each step of a research design Combining steps to form a design Simulating a research design Diagnosing a research design Comparing designs   DeclareDesign is a system for describing research designs in code and simulating them in order to understand their properties. Because DeclareDesign employs a consistent grammar of designs, you can focus on the intellectually challenging part – designing good research studies – without having to code up simulations from scratch.</description>
    </item>
    
    <item>
      <title>Model-Inquiry-Data Strategy-Answer Strategy (MIDA)</title>
      <link>/mida.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/mida.html</guid>
      <description>The idea motivating DeclareDesign is that the core analytic features of research designs can be declared in a complete manner and saved as an object. Once properly declared, a design can easily be shared, modified, improved, and used. A design contains the information needed to implement key parts of data generation and subsequent analysis. It also contains enough information to allow researchers or third parties to query it and determine whether it can support the claims it makes.</description>
    </item>
    
    <item>
      <title>Pre-analysis plans with DeclareDesign</title>
      <link>/pap.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/pap.html</guid>
      <description>Design declaration in four languages  Language Declaration in code Figure based on mock data Diagnosis    R     Stata     Python     Excel       Click on the declarations to download the code files; the figures to download the code that generated them; and the diagnosis to download a reproducible document that includes the diagnosis.</description>
    </item>
    
  </channel>
</rss>
