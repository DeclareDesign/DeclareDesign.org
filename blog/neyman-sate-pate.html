<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.79.0" />


  <title>Common estimators of uncertainty overestimate uncertainty - DeclareDesign</title>




  









<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css'>



<script defer src="https://use.fontawesome.com/releases/v5.5.0/js/all.js" integrity="sha384-GqVMZRt5Gn7tB9D9q7ONtcp4gtHIUEW/yG7h98J7IpE3kpi+srfFyyB/04OV6pG0" crossorigin="anonymous"></script>

<link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,500,500i,700,700i" rel="stylesheet">

<link rel="stylesheet" href="/css/bootstrap.min.css">

<script src="https://code.jquery.com/jquery-3.3.1.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>






  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Common estimators of uncertainty overestimate uncertainty">
  <meta name="twitter:description" content='Random assignment provides a justification not just for estimates of effects but also for estimates of uncertainty about effects. The basic approach, due to Neyman, is to estimate the variance in estimates of the difference between outcomes in treatment and in control outcomes using the variability that can be observed among units in control and units in treatment. It’s an ingenious approach and dispenses with the need to make any assumptions about the shape of statistical distributions or about asymptotics. The problem though is that it can sometimes be upwardly biased, meaning that it might lead you to maintain null hypotheses when you should be rejecting them. We use design diagnosis to get a handle on how great this problem is and how it matters for different estimands.

'>






<script src="/js/dropdown_menu.js"></script>
<link rel="stylesheet" href="/css/custom.css">

  </head>
  <body>
    <div class="wrapper">
      
        
<header>
  <div class="navbar-wrap fixed-top bg-white">
    <nav class="navbar navbar-expand-lg navbar-light">
      <div class="container-fluid"> <a class="navbar-brand" href="/"><img src="/images/brand.svg" alt=""></a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"> <span class="navbar-toggler-icon"></span> </button>
        <div class="collapse navbar-collapse" id="navbarSupportedContent">
          <ul class="navbar-nav ml-auto">
            <li class="nav-item"><a class="nav-link" href="/getting-started.html">Getting Started</a></li>
            <li class="nav-item"><a class="nav-link" href="/library.html">Library</a></li>
            <li class="nav-item dropdown">
              
              <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Software <b class="caret"></b></a>
              <ul class="dropdown-menu">
                <li><a class="dropdown-item" href="/r/declaredesign/">DeclareDesign</a></li>
                <li><a class="dropdown-item" href="/r/randomizr/">randomizr</a></li>
                <li><a class="dropdown-item" href="/r/fabricatr/">fabricatr</a></li>
                <li><a class="dropdown-item" href="/r/estimatr/">estimatr</a></li>
                <li><a class="dropdown-item" href="/r/designlibrary/">DesignLibrary</a></li>
              </ul>
            </li>
            <li class="nav-item"><a class="nav-link" href="/blog.html">Blog</a></li>
            <li class="nav-item"><a class="nav-link" href="/about.html">About</a></li>
            <li class="nav-item"><a target="_blank" class="nav-link" href="http://discuss.declaredesign.org/">Help</a></li>
          </ul>
        </div>
      </div>
    </nav>
  </div>
</header>

 

      


<main class="container">
  <div class="row justify-content-between">
    <div class="col-lg-12" id="content_column">
      <article class="article">

        <a class="h1 d-block mb-3" href="/blog">DeclareDesign Blog</a>
        <h2 class="article-title">Common estimators of uncertainty overestimate uncertainty</h2>

        
        <span class="article-date">2018/11/07</span>
        

        <div class="article-content">
          
<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>


<p>Random assignment provides a justification not just for estimates of effects but also for estimates of uncertainty about effects. The basic approach, due to Neyman, is to estimate the variance in estimates of the difference between outcomes in treatment and in control outcomes using the variability that can be observed among units in control and units in treatment. It’s an ingenious approach and dispenses with the need to make any assumptions about the shape of statistical distributions or about asymptotics. The problem though is that it can sometimes be <strong>upwardly biased</strong>, meaning that it might lead you to maintain null hypotheses when you should be rejecting them. We use design diagnosis to get a handle on how great this problem is and how it matters for different estimands.</p>
<p>The key insight is that how great a problem this is depends on the target of inference. If you are targeting a <strong>sample</strong> average treatment effect (SATE), your standard errors will often be too big. If you are targeting a <strong>population</strong> average treatment effect (PATE), but that population is itself sampled from a superpopulation, your standard errors may also be too big, though the issues are less severe. If you’re targeting a <strong>superpopulation</strong> average treatment effect (SPATE), your standard errors will be the right size.</p>
<p>A quick word on what we’re <strong>not</strong> talking about. Under complete random assignment (exactly <span class="math inline">\(m\)</span> of <span class="math inline">\(N\)</span> units in the sample are treated), the difference-in-means estimator of the average treatment effect is of course <strong>unbiased</strong>, which means that it returns exactly the right answer, on average. The bias we are talking about here isn’t in the estimator of the treatment effect, but instead, it’s in the standard error estimator.</p>
<p>We want to estimate <span class="math inline">\(\text{sd}\left(\frac{1}{n_T}\sum_{i \in T} Y_i(1) - \frac{1}{n_C}\sum_{i \in C} Y_i(0) \right)\)</span>. The usual estimator for this is the Neyman standard error. This estimator uses the fact that the variance of a difference (treatment - control) can be expressed in terms of the variance of each group (variance of treatment outcomes, variance of control outcomes), and the covariance between them (see <a href="https://projecteuclid.org/download/pdfview_1/euclid.aoas/1206367817">Freedman</a>). We can estimate the variances easily enough but unfortunately we cannot estimate the covariance because we never simultaneously see outcomes for treatment and control for a given unit. Instead the Neyman estimator assumes the worst and estimates an upper bound on the variance. This is equivalent to HC2 robust standard errors in a regression setup and it is what is estimated by default in <code>estimatr::difference_in_means</code> and <code>estimatr::lm_robust</code>.</p>
<p>From some perspectives, a conservative standard error estimator is better than the alternative. With a conservative estimator, we are less certain of our ATE estimates than we should be. If you’re conducting hypothesis tests, upward bias in the standard errors means you’re less likely to commit a Type I error (inappropriately rejecting the null) but you’re more likely to commit a Type II error (inappropriately failing to reject). So how bad this is depends on how you trade off these errors.</p>
<p>We’re going to investigate this question at three levels that correspond to three estimands. The highest level is the <strong>superpopulation</strong>, and the associated estimand is the superpopulation average treatment effect (SPATE). The next highest is the <strong>population</strong>, whose estimand is the population average treatment effect (PATE). The lowest level is the <strong>sample</strong>, with an associated estimand called the sample average treatment effect (SATE). Depending on how units are sampled into the population or the sample, these three estimands could have different values.</p>
<p>We’re interested in:</p>
<ol style="list-style-type: decimal">
<li>The <strong>true</strong> standard deviation of the sampling distribution of estimates for the SPATE, the PATE, and the SATE.</li>
<li>The <strong>estimated</strong> standard errors associated with each estimand.</li>
</ol>
<p>We can learn about all of these using a simulation conducted in <code>DeclareDesign</code>. The design includes the parameters <code>r</code> (the correlation between Y0 and Y1) and <code>b</code> (the superpopulation average treatment effect) in case you’d like to vary them, but we’ve set them to zero for this demonstration. Note that setting <code>r=0</code> means that potential outcomes under treatment and control are not correlated. This implies heterogeneous effects: if effects were homogeneous then these outcomes would be perfectly correlated (for intuition, imagine a graph of <span class="math inline">\(Y(1)\)</span> against <span class="math inline">\(Y(0)\)</span>).</p>
<pre class="r"><code>r &lt;- 0 # correlation between Y0 and Y1
b &lt;- 0 # average treatment effect

design &lt;- 
  declare_population(N = 64,
                     u_0 = rnorm(N),
                     u_1 = rnorm(n = N, mean = r * u_0, sd = sqrt(1 - r^2))) +
  declare_potential_outcomes(Y_Z_0 = u_0, 
                             Y_Z_1 = u_1 + b) +
  declare_sampling(n = 32) +
  declare_assignment(prob = 0.5) +
  declare_reveal(Y, Z) +
  declare_estimator(Y ~ Z)</code></pre>
<div id="the-sate" class="section level1">
<h1>The SATE</h1>
<p>Let’s start with the bottom level, the sample. If the target is the SATE, that means we want to understand the sampling distribution of the ATE estimator, <strong>conditional</strong> on the sample. We take advantage of a cool feature in <code>DeclareDesign</code> that allows you to control exactly how many draws are conducted at each step by passing a <strong>vector</strong> to the <code>sims</code> argument. If you said <code>sims = c(1, 2, 3)</code>, we would simulate the first step once, the second step twice (<span class="math inline">\(1 \times 2\)</span>), and the third step six times (<span class="math inline">\(1 \times 2 \times 3\)</span>).</p>
<p>To simulate the sampling distribution of the SATE estimator, given a particular sample, we want:</p>
<ul>
<li>1 draw each from the <code>declare_population()</code>, <code>declare_potential_outcomes()</code>, and <code>declare_sampling()</code> steps (together, a single draw of a sample from the population with its potential outcomes)</li>
<li>A bunch of draws from the <code>declare_assignment</code> step (assigning treatment over and over within the fixed sample). For this simulation, we’ve set <code>assignment_draws</code> to 500</li>
<li>1 draw each from the <code>declare_reveal</code> and <code>declare_estimator</code> steps</li>
</ul>
<pre class="r"><code>SATE_sims &lt;- simulate_design(design, sims = c(1, 1, 1, assignment_draws, 1, 1))</code></pre>
<p>This code summarizes the simulations and calculates (1) the true variance of the sampling distribution and (2) the average estimated standard error squared. The bias of standard error estimators is usually discussed in terms of variance, not the square root of the variance, because the square root operation is non-linear.</p>
<p>The figure shows the distribution of estimates that you would get given a particular sample.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> As we can see, the true variance is lower than the average estimated variance, indicating <strong>upward</strong> bias in this case.</p>
<pre class="r"><code>SATE_summary_df &lt;-
  SATE_sims %&gt;% 
  summarise(`True Variance of SATE estimator` = var(estimate),
            `Average Estimated Standard Error Squared` = mean(std.error ^2)) %&gt;%
  gather(diagnosand, value)</code></pre>
<pre class="r"><code>SATE_summary_df</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">diagnosand</th>
<th align="right">value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">True Variance of SATE estimator</td>
<td align="right">0.065</td>
</tr>
<tr class="even">
<td align="left">Average Estimated Standard Error Squared</td>
<td align="right">0.120</td>
</tr>
</tbody>
</table>
<p>We can also plot the sampling distribution of the standard error estimator to visually show the bias.</p>
<pre class="r"><code>SATE_sims %&gt;%
  ggplot(aes(std.error^2)) +
  geom_histogram(bins = 30) +
  geom_vline(data = SATE_summary_df, aes(xintercept = value, color = diagnosand), size = 2) + 
  guides(colour = guide_legend(reverse = TRUE))</code></pre>
<p><img src="/blog/2018-11-06-Neyman-SATE-PATE_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
</div>
<div id="the-pate" class="section level1">
<h1>The PATE</h1>
<p>Next, let’s go one level up. Now the <code>sims</code> argument shows that we’re drawing a single population, then repeatedly drawing samples from that population. The graph shows that even in this case, the true variance of the PATE estimator is lower than estimated.</p>
<pre class="r"><code>PATE_sims &lt;- simulate_design(design, sims = c(1, 1, 500, 1, 1, 1))</code></pre>
<p><img src="/blog/2018-11-06-Neyman-SATE-PATE_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
</div>
<div id="the-spate" class="section level1">
<h1>The SPATE</h1>
<p>But if we imagine we’re sampling from an infinite <strong>superpopulation</strong>, things look better. Here we repeatedly draw a population from the superpopulation, then draw a sample from that population, and then assign treatment.</p>
<pre class="r"><code>SPATE_sims &lt;- simulate_design(design, sims = c(500, 1, 1, 1, 1, 1))</code></pre>
<p><img src="/blog/2018-11-06-Neyman-SATE-PATE_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
</div>
<div id="some-intuition" class="section level1">
<h1>Some Intuition</h1>
<p>Why does this happen?</p>
<p>Say a bag contains half white stones and half black stones and I pull one stone and have to guess about the value of a second stone. If there are only two stones in the bag, then I can use the first draw to guess perfectly about the color of the second one. I can make a reasonably informed guess if there are four stones in the bag. But if there are an infinity of stones then my first draw tells me nothing about the second draw.</p>
<p>In essence this is what is happening here. To make the link to treatment effects imagine there were just two units. Unit 1 has outcomes <span class="math inline">\(Y_1(0) = 0, Y_1(1) = 1\)</span> and unit 2 has <span class="math inline">\(Y_2(0) = 1, Y_2(1) = 2\)</span>. Then both units have a +1 treatment effect and potential outcomes are positively correlated. Depending on which unit is assigned to treatment the estimate will be either 0 or 2. So quite a bit of variability despite the homogeneous effects. The reason is that if a unit with high outcomes enters the treatment group, then that same unit (with high values) will be missing from the control group and so unusually high outcomes in the treatment group correspond to low outcomes in the control group. Imagine instead that <span class="math inline">\(Y_1(0) = 0, Y_1(1) = 2\)</span> and unit 2 has <span class="math inline">\(Y_2(0) = 1, Y_2(1) = 1\)</span>. Then the average effect is still +1 but there are heterogeneous effects and potential outcomes are negatively correlated. No matter which unit is assigned to treatment the estimate will be 1. So no variability now. The reason is that if a unit with a high outcome enters the treatment group, then that same unit (with low values) will be missing from the control group and so both the treatment and control groups will have positive shocks that get differenced out.</p>
<p>The key thing is that there are reductions in variability that arise, given heterogeneous effects, when assignments into one group have implications for values in the other. But <em>that implication does not hold so strongly when units are sampled from bigger populations</em> (in the same way as knowing the color of a stone from one draw is less informative about a second draw the bigger the bag from which you are drawing).</p>
<p>Another way to think about this is that each level of sampling (from superpopulation to population, from population to sample, from sample to treatment sampling) adds a layer of uncertainty to the <strong>true</strong> sampling distribution of the estimates. So the distribution of the SATE estimator is tighter than the distribution of PATE estimator is tighter than the distribution of the SPATE estimator. But the standard error estimator is the <em>same</em> in all three cases – it assumes a worst case scenario… which happens to only be true in the SPATE case. In the SATE and PATE cases, the Neyman standard error estimator is upwardly biased.</p>
</div>
<div id="implications-for-inference" class="section level1">
<h1>Implications for Inference</h1>
<p>We’re often concerned that because of <span class="math inline">\(p\)</span>-hacking and specification searches, reported <span class="math inline">\(p\)</span>-values are too low. Funnily enough, the fact that the Neyman standard error estimator is conservative means that sometimes, the reported <span class="math inline">\(p\)</span>-values may be too high!</p>
</div>
<div id="further-reading" class="section level1">
<h1>Further reading</h1>
<p>It is possible to do better than the Neyman estimate by putting upper and lower <em>bounds</em> on the standard error. See <span class="citation">Aronow et al. (2014)</span> ( <a href="https://projecteuclid.org/euclid.aos/1400592645">ungated</a> ) for a lovely treatment of these issues.</p>
<div id="refs" class="references">
<div id="ref-aronow2014sharp">
<p>Aronow, Peter M, Donald P Green, Donald KK Lee, and others. 2014. “Sharp Bounds on the Variance in Randomized Experiments.” <em>The Annals of Statistics</em> 42 (3): 850–71.</p>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>If you were interested in how the bias of the estimated variances is distributed <em>over</em> samples then you would want to this operation multiple times, which is easily done using <code>sims = c(500, 1, 1, 500, 1, 1)</code><a href="#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>

        </div>
      </article>
      



    </div>
  </div>
</main>

    </div>

    

<footer>
    <div class="footer-top">
      <div class="container">
        <div class="row no-gutters">
          <div class="col-lg-7 col-md-12">
            <div class="row">
              <div class="col-md-3 col-12">
                <div class="footer-brand"><a href=""><img src="/images/brand-footer.svg" alt=""></a></div>
              </div>
              <div class="col-md-3 col-4">
                <p>DeclareDesign</p>
                <ul class="list-unstyled">
                  <li><a href="/">Home</a></li>
                  <li><a href="/about">About</a></li>
                  <li><a href="/library">Library</a></li>
                  <li><a href="/blog">Blog</a></li>
                  <li><a href="http://discuss.declaredesign.org/">Contact</a></li>
                  <li><a href="/funding">Funding</a></li>
                  <li><a href="http://discuss.declaredesign.org/">Help</a></li>
                </ul>
              </div>
              <div class="col-md-3 col-4">
                <p>Software</p>
                <ul class="list-unstyled">
                  <li><a href="/r/declaredesign/">DeclareDesign for R</a></li>
                  <li><a href="/r/estimatr/">estimatr for R</a></li>
                  <li><a href="/r/randomizr/">randomizr for R</a></li>
                  <li><a href="/stata/randomizr/">randomizr for Stata</a></li>
                  <li><a href="/r/fabricatr/">fabricatr for R</a></li>
                </ul>
              </div>
              <div class="col-md-3 col-3">
                <p>Contributing</p>
                <ul class="list-unstyled">
                  <li><a target="_blank" href="http://github.com/declareDesign/">github</a></li>
                  <li><a href="">twitter</a></li>
                  <li><a href="">paper</a></li>
                  <li><a href="">book</a></li>
                </ul>
              </div>
            </div>
          </div>
          <div class="col-lg-5 col-md-12"></div>
        </div>
      </div>
    </div>
    <div class="footer-bottom">
      <div class="footer-meta-block">
        <div class="container">
          <div class="row align-items-center">
            <div class="col-sm-7">
              <p class="mb-0">© 2019 <a href="https://declaredesign.org">DeclareDesign.org</a>. All rights reserved.</p>
            </div>
            <div class="col-sm-5">
              <div class="fbl-link-wrap">
                <ul class="list-inline mb-0">
                  <li class="list-inline-item"> <a target="_blank" href="http://github.com/declareDesign/"><img src="images/icon-git.svg" alt=""></a> </li>
                </ul>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </footer>
  
   
  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy" crossorigin="anonymous"></script>

    <link rel="stylesheet" href="https://cdn.datatables.net/v/bs4/dt-1.10.18/datatables.min.css"/>
<script src="https://cdn.datatables.net/v/bs4/dt-1.10.18/datatables.min.js"></script>

<script>
    jQuery(function ()
    {
        const library_list = jQuery("#design_library_list");
        library_list.addClass("table table-striped table-bordered");
        library_list.DataTable(
            {
                "autoWidth": false,
                "columns":
                    [
                        {"width": "30%"}, 
                        {"width": "10%"}, 
                        {"width": "10%"}, 
                        {"width": "10%"}, 
                        {"width": "15%"}, 
                        {"width": "25%"}, 
                    ],
                "drawCallback": initialize_tooltips
            }
        );

        library_list.css("width", "");
    });

    function initialize_tooltips()
    {
        const tooltip_elements = jQuery('[data-toggle="tooltip"]');
        tooltip_elements.tooltip();
        tooltip_elements.click(function ()
        {
            jQuery(this).tooltip("hide");
        });
    }
</script>

    


<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>
jQuery(function ()
{
  $('pre').each(function (index)
  {
    hljs.highlightBlock(this);
  });
});
</script>



    
<script src="/js/math-code.js"></script>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    



    
  </body>
</html>

