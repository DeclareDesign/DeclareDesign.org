<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.79.0" />


  <title>Randomization does not justify t-tests. How worried should I be? - DeclareDesign</title>




  









<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css'>



<script defer src="https://use.fontawesome.com/releases/v5.5.0/js/all.js" integrity="sha384-GqVMZRt5Gn7tB9D9q7ONtcp4gtHIUEW/yG7h98J7IpE3kpi+srfFyyB/04OV6pG0" crossorigin="anonymous"></script>

<link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,500,500i,700,700i" rel="stylesheet">

<link rel="stylesheet" href="/css/bootstrap.min.css">

<script src="https://code.jquery.com/jquery-3.3.1.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>






  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Randomization does not justify t-tests. How worried should I be?">
  <meta name="twitter:description" content='Deaton and Cartwright (2017) provide multiple arguments against claims that randomized trials should be thought of as a kind of gold standard of scientific evidence. One striking argument they make is that randomization does not justify the statistical tests that researchers typically use. They are right in that. Even if researchers can claim that their estimates of uncertainty are justified by randomization, their habitual use of those estimates to conduct t-tests are not. To get a handle on how severe the problem is we replicate the results in Deaton and Cartwright (2017) and then use a wider set of diagnosands to probe more deeply. Our investigation suggests that what at first seems like a big problem might not in fact be so great if your hypotheses are what they often are for experimentalists—sharp and sample-focused.

'>






<script src="/js/dropdown_menu.js"></script>
<link rel="stylesheet" href="/css/custom.css">

  </head>
  <body>
    <div class="wrapper">
      
        
<header>
  <div class="navbar-wrap fixed-top bg-white">
    <nav class="navbar navbar-expand-lg navbar-light">
      <div class="container-fluid"> <a class="navbar-brand" href="/"><img src="/images/brand.svg" alt=""></a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"> <span class="navbar-toggler-icon"></span> </button>
        <div class="collapse navbar-collapse" id="navbarSupportedContent">
          <ul class="navbar-nav ml-auto">
            <li class="nav-item"><a class="nav-link" href="/getting-started.html">Getting Started</a></li>
            <li class="nav-item"><a class="nav-link" href="/r/designlibrary">Library</a></li>
            <li class="nav-item dropdown">
              
              <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Software <b class="caret"></b></a>
              <ul class="dropdown-menu">
                <li><a class="dropdown-item" href="/r/declaredesign/">DeclareDesign</a></li>
                <li><a class="dropdown-item" href="/r/randomizr/">randomizr</a></li>
                <li><a class="dropdown-item" href="/r/fabricatr/">fabricatr</a></li>
                <li><a class="dropdown-item" href="/r/estimatr/">estimatr</a></li>
                <li><a class="dropdown-item" href="/r/designlibrary/">DesignLibrary</a></li>
              </ul>
            </li>
            <li class="nav-item"><a class="nav-link" href="/blog.html">Blog</a></li>
            <li class="nav-item"><a class="nav-link" href="/about.html">About</a></li>
            <li class="nav-item"><a target="_blank" class="nav-link" href="http://discuss.declaredesign.org/">Help</a></li>
          </ul>
        </div>
      </div>
    </nav>
  </div>
</header>

 

      


<main class="container">
  <div class="row justify-content-between">
    <div class="col-lg-12" id="content_column">
      <article class="article">

        <a class="h1 d-block mb-3" href="/blog">DeclareDesign Blog</a>
        <h2 class="article-title">Randomization does not justify t-tests. How worried should I be?</h2>

        
        <span class="article-date">2018/11/27</span>
        

        <div class="article-content">
          
<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>


<p><span class="citation">Deaton and Cartwright (2017)</span> provide multiple arguments against claims that randomized trials should be thought of as a kind of gold standard of scientific evidence. One striking argument they make is that randomization does not justify the statistical tests that researchers typically use. They are right in that. Even if researchers can claim that their estimates of uncertainty are justified by randomization, their habitual use of those estimates to conduct <em>t</em>-tests are not. To get a handle on how severe the problem is we replicate the results in <span class="citation">Deaton and Cartwright (2017)</span> and then use a wider set of diagnosands to probe more deeply. Our investigation suggests that what at first seems like a big problem might not in fact be so great if your hypotheses are what they often are for experimentalists—sharp and sample-focused.</p>
<p>More specifically, <span class="citation">Deaton and Cartwright (2017)</span> argue that that “spurious significance […] arises when the distribution of treatment effects contains outliers or, more generally, is not symmetric.” They back up the claim with simulation results from a case with heterogeneous asymmetrically distributed treatment effects that center on 0. In fact however, both the sharp null of no effect and the null of no average effect in a given sample are false in this example and so the worry about over-rejecting does not apply to these hypotheses.</p>
<p>A bit more generally, <em>under the sharp null of no effect</em> the distribution of treatment effects in simple trials (with 50% assignment probabilities) will be perfectly symmetrical even if the distribution of potential outcomes is arbitrarily skewed and so the concern <span class="citation">Deaton and Cartwright (2017)</span> point to doesn’t arise in the first place.</p>
<p>The most important take away though might be that it’s hard to think through when the use of <em>t</em>-tests is or is not appropriate. But design diagnosis can tip you off to whether this likely a problem in your case.</p>
<div id="the-deaton-and-cartwright-example" class="section level1">
<h1>The Deaton and Cartwright Example</h1>
<p>The design described by Deaton and Cartwright can be declared quite compactly:</p>
<pre class="r"><code>N = 50

dc_design &lt;-
  declare_population(N = N, u = rlnorm(N) - exp(.5)) +
  declare_potential_outcomes(Y_Z_0 = 0, Y_Z_1 = u) +
  declare_estimand(SPATE = 0, SATE = mean(Y_Z_1 - Y_Z_0)) +
  declare_assignment(prob = .5) +
  declare_reveal(Y, Z) +
  declare_estimator(Y ~ Z, estimand = c(&quot;SPATE&quot;, &quot;SATE&quot;)) </code></pre>
<p>The key thing in this design is that the treatment effects have a very skewed distribution (distribution log normal, though recentered on 0).</p>
<p>Let’s look at the power of this design over a range of N’s. Power is a key diagnosand here because Deaton and Cartwright’s position is that, since the true average treatment effect is 0, the power should be 0.05—that is, you should only reject a true null 5% of the time if you are using a 5% significance cutoff.</p>
<p>Here we run a diagnosis over a range of designs:</p>
<pre class="r"><code>diagnosis &lt;- diagnose_design(redesign(dc_design, N = c(50, 100, 200, 400, 600, 800, 1000)))</code></pre>
<p>and plot the results:</p>
<pre class="r"><code>get_diagnosands(diagnosis) %&gt;%
  mutate(N = as.numeric(paste(N))) %&gt;%
  ggplot(aes(N, power)) +
  geom_line() +
  facet_wrap(~estimand_label)</code></pre>
<p><img src="/blog/2018-11-27-deaton_cartwright_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<!-- wilcox_test <- function(data) -->
<!--   data.frame(tidy(difference_in_means(Y~Z, data = data)), -->
<!--             p.wilcox = wilcox.test(Y~Z, data = data)$p.value) -->
<!-- my_estimator_custom <- declare_estimator(handler = label_estimator(wilcox_test)) -->
<p>As advertised, power appears too high with the small <span class="math inline">\(N\)</span> cases. It declines towards 0.05 as <span class="math inline">\(N\)</span> increases, though it is still somewhat greater than 0.05 even for reasonably sized studies. The same pattern holds whether we are targeting the superpopulation average treatment effect (SPATE) or the sample average treatment effect (SATE), because the estimator is identical in both cases.</p>
</div>
<div id="a-closer-look" class="section level1">
<h1>A closer look</h1>
<p>If you plot the estimates and the estimands from the simulations you will notice an interesting feature of the Deaton and Cartwright case. Although the true SPATE is always zero, the SATE for any particular finite sample is <em>not</em> zero. Here are scatterplots of the joint distribution of estimands and estimates for two sample sizes. The true value of the SPATE is always zero, but the true value of the SATE is never <em>exactly</em> zero. You can see that the distribution of estimates is highly skewed (most easily seen for the SPATE facet when N = 50).</p>
<pre class="r"><code>get_simulations(diagnosis) %&gt;%
  ggplot(aes(estimand, estimate)) +
  geom_point(alpha = 0.05, size = 0.5, stroke = 0) +
  facet_grid(N~estimand_label)</code></pre>
<p><img src="/blog/2018-11-27-deaton_cartwright_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>The fact that the true value of the SATE is never zero has implications for the interpretation of power, but also for a range of other diagnosands. In addition to power, coverage, the true standard error, the average estimated standard error, and the type-S rate (probability that a significant result is of the wrong sign), we’ll calculate the variance of the estimate, the average estimated variance, and the variance of the estimands themselves.</p>
<pre class="r"><code>diagnosands &lt;- declare_diagnosands(select = c(&quot;power&quot;, &quot;coverage&quot;, &quot;sd_estimate&quot;, &quot;mean_se&quot;, &quot;type_s_rate&quot;),
                                   var_estimate = var(estimate), 
                                   est_var_est = mean(std.error^2),
                                   var_estimand = var(estimand))</code></pre>
<p>Now calculate the diagnosands for the small design:</p>
<pre class="r"><code>diagnosis &lt;- diagnose_design(dc_design, diagnosands = diagnosands)</code></pre>
<table>
<colgroup>
<col width="14%" />
<col width="12%" />
<col width="11%" />
<col width="12%" />
<col width="6%" />
<col width="8%" />
<col width="11%" />
<col width="7%" />
<col width="11%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Estimand Label</th>
<th align="left">Var Estimate</th>
<th align="left">Est Var Est</th>
<th align="left">Var Estimand</th>
<th align="left">Power</th>
<th align="left">Coverage</th>
<th align="left">SD Estimate</th>
<th align="left">Mean Se</th>
<th align="left">Type S Rate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">SATE</td>
<td align="left">0.19</td>
<td align="left">0.18</td>
<td align="left">0.09</td>
<td align="left">0.13</td>
<td align="left">0.94</td>
<td align="left">0.43</td>
<td align="left">0.38</td>
<td align="left">0.11</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left">(0.00)</td>
<td align="left">(0.00)</td>
<td align="left">(0.00)</td>
<td align="left">(0.00)</td>
<td align="left">(0.00)</td>
<td align="left">(0.00)</td>
<td align="left">(0.00)</td>
<td align="left">(0.01)</td>
</tr>
<tr class="odd">
<td align="left">SPATE</td>
<td align="left">0.19</td>
<td align="left">0.18</td>
<td align="left">0.00</td>
<td align="left">0.13</td>
<td align="left">0.87</td>
<td align="left">0.43</td>
<td align="left">0.38</td>
<td align="left">1.00</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left">(0.00)</td>
<td align="left">(0.00)</td>
<td align="left">(0.00)</td>
<td align="left">(0.00)</td>
<td align="left">(0.00)</td>
<td align="left">(0.00)</td>
<td align="left">(0.00)</td>
<td align="left">(0.00)</td>
</tr>
</tbody>
</table>
<p>A few things are worth noting here.</p>
<ul>
<li><p>First, estimates of <em>variance</em> do pretty well. In this case there is no covariance between potential outcomes and so the <a href="https://declaredesign.org/blog/neyman-sate-pate.html">Neyman approach</a> does fine. The Neyman approach doesn’t make any assumptions about the shape of distributions and so the skew makes no difference here.</p></li>
<li><p>Second, the standard error estimates – the square root of the variance estimates – are too low. This is because the square root function is a nonlinear transformation. Unbiasedness of the variance does not imply unbiasedness of the standard deviation.</p></li>
<li><p>Third, coverage is off. Confidence intervals depend on the use of the <span class="math inline">\(t\)</span>-distribution, which is in question here. Note though that the problem is particularly acute for SPATE; coverage for the SATE seems very good.</p></li>
<li><p>Fourth, the <em>type S</em> error rate is really worth looking at. The type S rate reports the probability that a significant result is of the wrong sign. For the superpopulation estimand—the SPATE—which we assume to be 0, the sign is <em>always</em> wrong. But for the SATE—the sample average treatment effect—the sign is rarely wrong. In fact the probability of getting a wrongly signed significant estimate is <em>power x type_s_rate = very very small</em>.</p></li>
</ul>
<p>This forces a rethinking of the results. Looking at this evidence it seems like the real worry, perhaps, is that we <em>fail</em> to reject the null so often, given that is, in practice, never true. Conversely, from the SATE perspectives, most of these seemingly wrong results that worry Deaton and Cartwright are really correct results.</p>
<p>Should a researcher coming at this from a design-based perspective worry? Researchers using design-based inference might not approach the hypothesis test using a <em>t</em>-test but might instead use randomization inference. Unlike the <em>t</em>-test, the <a href="https://egap.org/methods-guides/10-things-randomization-inference">randomization inference</a> approach <em>is</em> justified by the randomization. In doing so though, they would likely test what’s called the <em>sharp null</em>—the hypothesis that the effect is zero not just on average <em>but for all units</em>. Interestingly in this example, the sharp null is false not just for all finite samples, but also for the superpopulation. Looking at all this from the randomization inference perspective, one might again worry not that there is too much rejection of the null but that there is insufficient rejection of the null.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<p>Thus, a fuller diagnosis suggests that while power may be too high for a weak null in the superpopulation, it is regrettably <em>low</em> for the kind of sample-based hypotheses or sharp nulls that experimentalists often focus on.</p>
</div>
<div id="codas" class="section level1">
<h1>Codas</h1>
<p>We get a little further into the weeds here.</p>
<div id="there-is-no-skew-when-sharp-nulls-are-true" class="section level2">
<h2>There is no skew when <em>sharp</em> nulls are true</h2>
<p>For the record, we can do the same analysis when the sharp null is in fact true. This just requires replacing the potential outcomes step (step 2) in the design.</p>
<pre class="r"><code>dc_sharp &lt;- replace_step(dc_design, 2, 
                         declare_potential_outcomes(Y_Z_0 = u, Y_Z_1 = u))
diagnose_design(dc_sharp)</code></pre>
<table>
<colgroup>
<col width="14%" />
<col width="12%" />
<col width="11%" />
<col width="12%" />
<col width="6%" />
<col width="8%" />
<col width="11%" />
<col width="7%" />
<col width="11%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Estimand Label</th>
<th align="left">Var Estimate</th>
<th align="left">Est Var Est</th>
<th align="left">Var Estimand</th>
<th align="left">Power</th>
<th align="left">Coverage</th>
<th align="left">SD Estimate</th>
<th align="left">Mean Se</th>
<th align="left">Type S Rate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">SATE</td>
<td align="left">0.38</td>
<td align="left">0.37</td>
<td align="left">0.00</td>
<td align="left">0.04</td>
<td align="left">0.96</td>
<td align="left">0.61</td>
<td align="left">0.56</td>
<td align="left">1.00</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left">(0.01)</td>
<td align="left">(0.00)</td>
<td align="left">(0.00)</td>
<td align="left">(0.00)</td>
<td align="left">(0.00)</td>
<td align="left">(0.01)</td>
<td align="left">(0.00)</td>
<td align="left">(0.00)</td>
</tr>
<tr class="odd">
<td align="left">SPATE</td>
<td align="left">0.38</td>
<td align="left">0.37</td>
<td align="left">0.00</td>
<td align="left">0.04</td>
<td align="left">0.96</td>
<td align="left">0.61</td>
<td align="left">0.56</td>
<td align="left">1.00</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left">(0.01)</td>
<td align="left">(0.00)</td>
<td align="left">(0.00)</td>
<td align="left">(0.00)</td>
<td align="left">(0.00)</td>
<td align="left">(0.01)</td>
<td align="left">(0.00)</td>
<td align="left">(0.00)</td>
</tr>
</tbody>
</table>
<p>We find that we do not see the same issue arise when in fact the sharp null is true (in the superpopulation, and thus in every sample). With a true sharp null (and .5 assignment probabilities), even if the potential outcomes are very skewed, the distribution of estimated effects will be symmetrical for the simple reason that, for any estimated treatment effect <span class="math inline">\(\hat{\tau}\)</span> arising from assignment <span class="math inline">\(Z\)</span>, assignment <span class="math inline">\((1-Z)\)</span> yields <span class="math inline">\((-\hat{\tau})\)</span>. This clarifies that the skew-based concern about over-rejecting a null that Deaton and Cartwright raise actually depends on the sharp null being false in the first place. (Though, to be clear, the assumption of 50% assignment matters here — skew in estimated effects is certainly possible under the sharp null with other assignment probabilities.)</p>
</div>
<div id="but-there-can-still-be-dragons-so-then-what-to-do" class="section level2">
<h2>But there can still be dragons, so then what to do?</h2>
<p>Although we might not have to worry about skew when sharp nulls are true, <span class="math inline">\(t\)</span>-stats might still lead you astray when tails are fat.</p>
<p>As a simple example, imagine a world in which <span class="math inline">\(Y = 0\)</span> for 50 units and <span class="math inline">\(Y = 1\)</span> for 50 units, independent of <span class="math inline">\(Z\)</span>. Say <span class="math inline">\(Z\)</span> is randomly assigned to just four units. Whenever <span class="math inline">\(Z\)</span> is assigned to four units with the same value on <span class="math inline">\(Y\)</span>, a <em>t</em>-test will suggest a significant difference (<span class="math inline">\(p = 0.04\)</span>). You can interpret that as a claim that such a data pattern should only be observed 4% of the time if the null is true. But you can figure out pretty quickly that you will see data patterns like this about one eighth of the time.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> So the probability of observing such data under the null is actually much higher than the 4% you might infer from the <span class="math inline">\(t\)</span>-test.</p>
<p>So in general there can be dangers using t-tests even with experimental data. A solution in this instance, if we are interested in a sharp null of no effect, is to do randomization inference. This would produce exactly the right <em>p</em>-value. But that will help only if you are alerted to the problem. To alert yourself to the problem, you could routinely diagnose a design with zero effects (a “null design”) and so <em>set yourself up to get a tip off when your power is too high</em>.</p>
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-deaton2017understanding">
<p>Deaton, Angus, and Nancy Cartwright. 2017. “Understanding and Misunderstanding Randomized Controlled Trials.” <em>Social Science &amp; Medicine</em>.</p>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Take a look at the <code>ri2</code> package for more on <a href="http://alexandercoppock.com/ri2/index.html">conducting randomization inference in R</a>.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>There is a roughly 50% chance the second unit will have the same <span class="math inline">\(Y\)</span> value as the first unit, roughly 50% chance that the third will be the same as the second, and roughly 50% chance the fourth the same as the third. More exactly: (49/99)x(48/98)x(47/97) = 0.117.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
</ol>
</div>

        </div>
      </article>
      



    </div>
  </div>
</main>

    </div>

    

<footer>
    <div class="footer-top">
      <div class="container">
        <div class="row no-gutters">
          <div class="col-lg-7 col-md-12">
            <div class="row">
              <div class="col-md-3 col-12">
                <div class="footer-brand"><a href=""><img src="/images/brand-footer.svg" alt=""></a></div>
              </div>
              <div class="col-md-3 col-4">
                <p>DeclareDesign</p>
                <ul class="list-unstyled">
                  <li><a href="/">Home</a></li>
                  <li><a href="/about.html">About</a></li>
                  <li><a href="/r/designlibrary">Library</a></li>
                  <li><a href="/blog.html">Blog</a></li>
                  <li><a href="http://discuss.declaredesign.org/">Help</a></li>
                </ul>
              </div>
              <div class="col-md-3 col-4">
                <p>Software</p>
                <ul class="list-unstyled">
                  <li><a href="/r/declaredesign/">DeclareDesign</a></li>
                  <li><a href="/r/estimatr/">estimatr</a></li>
                  <li><a href="/r/randomizr/">randomizr</a></li>
                  <li><a href="/r/fabricatr/">fabricatr</a></li>
                  <li><a href="/r/designlibrary/">DesignLibrary</a></li>
                </ul>
              </div>
              
            </div>
          </div>
          <div class="col-lg-5 col-md-12"></div>
        </div>
      </div>
    </div>
    <div class="footer-bottom">
      <div class="footer-meta-block">
        <div class="container">
          <div class="row align-items-center">
            <div class="col-sm-7">
              <p class="mb-0">&copy; 2021 Graeme Blair, Jasper Cooper, Alexander Coppock, and Macartan Humphreys</p>
            </div>
            <div class="col-sm-5">
              <div class="fbl-link-wrap">
                <ul class="list-inline mb-0">
                  <li class="list-inline-item"> <a target="_blank" href="http://github.com/DeclareDesign/"><img src="images/icon-git.svg" alt=""></a> </li>
                </ul>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </footer>
  
   
  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy" crossorigin="anonymous"></script>

    <link rel="stylesheet" href="https://cdn.datatables.net/v/bs4/dt-1.10.18/datatables.min.css"/>
<script src="https://cdn.datatables.net/v/bs4/dt-1.10.18/datatables.min.js"></script>

<script>
    jQuery(function ()
    {
        const library_list = jQuery("#design_library_list");
        library_list.addClass("table table-striped table-bordered");
        library_list.DataTable(
            {
                "autoWidth": false,
                "columns":
                    [
                        {"width": "30%"}, 
                        {"width": "10%"}, 
                        {"width": "10%"}, 
                        {"width": "10%"}, 
                        {"width": "15%"}, 
                        {"width": "25%"}, 
                    ],
                "drawCallback": initialize_tooltips
            }
        );

        library_list.css("width", "");
    });

    function initialize_tooltips()
    {
        const tooltip_elements = jQuery('[data-toggle="tooltip"]');
        tooltip_elements.tooltip();
        tooltip_elements.click(function ()
        {
            jQuery(this).tooltip("hide");
        });
    }
</script>

    


<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>
jQuery(function ()
{
  $('pre').each(function (index)
  {
    hljs.highlightBlock(this);
  });
});
</script>



    
<script src="/js/math-code.js"></script>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    



    
  </body>
</html>

