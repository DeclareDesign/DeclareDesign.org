<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.79.0" />


  <title>What can you learn from simulating qualitative inference strategies? - DeclareDesign</title>




  









<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css'>



<script defer src="https://use.fontawesome.com/releases/v5.5.0/js/all.js" integrity="sha384-GqVMZRt5Gn7tB9D9q7ONtcp4gtHIUEW/yG7h98J7IpE3kpi+srfFyyB/04OV6pG0" crossorigin="anonymous"></script>

<link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,500,500i,700,700i" rel="stylesheet">

<link rel="stylesheet" href="/css/bootstrap.min.css">

<script src="https://code.jquery.com/jquery-3.3.1.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>






  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="What can you learn from simulating qualitative inference strategies?">
  <meta name="twitter:description" content='Qualitative process-tracing sometimes seeks to answer “cause of effects” claims using within-case data: how probable is the hypothesis that (X) did in fact cause (Y)? Fairfield and Charman (2017), for example, ask whether the right changed position on tax reform during the 2005 Chilean presidential election ((Y)) because of anti-inequality campaigns ((X)) by examining whether the case study narrative bears evidence that you would only expect to see if this were true.1 When inferential logics are so clearly articulated, it becomes possible to do design declaration and diagnosis. Here we declare a Bayesian process-tracing design and use it to think through choices about what kinds of within-case information have the greatest probative value.

'>






<script src="/js/dropdown_menu.js"></script>
<link rel="stylesheet" href="/css/custom.css">

  </head>
  <body>
    <div class="wrapper">
      
        
<header>
  <div class="navbar-wrap fixed-top bg-white">
    <nav class="navbar navbar-expand-lg navbar-light">
      <div class="container-fluid"> <a class="navbar-brand" href="/"><img src="/images/brand.svg" alt=""></a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"> <span class="navbar-toggler-icon"></span> </button>
        <div class="collapse navbar-collapse" id="navbarSupportedContent">
          <ul class="navbar-nav ml-auto">
            <li class="nav-item"><a class="nav-link" href="/getting-started.html">Getting Started</a></li>
            <li class="nav-item"><a class="nav-link" href="/library.html">Library</a></li>
            <li class="nav-item dropdown">
              
              <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Software <b class="caret"></b></a>
              <ul class="dropdown-menu">
                <li><a class="dropdown-item" href="/r/declaredesign/">DeclareDesign</a></li>
                <li><a class="dropdown-item" href="/r/randomizr/">randomizr</a></li>
                <li><a class="dropdown-item" href="/r/fabricatr/">fabricatr</a></li>
                <li><a class="dropdown-item" href="/r/estimatr/">estimatr</a></li>
                <li><a class="dropdown-item" href="/r/designlibrary/">DesignLibrary</a></li>
              </ul>
            </li>
            <li class="nav-item"><a class="nav-link" href="/blog.html">Blog</a></li>
            <li class="nav-item"><a class="nav-link" href="/about.html">About</a></li>
            <li class="nav-item"><a target="_blank" class="nav-link" href="http://discuss.declaredesign.org/">Help</a></li>
          </ul>
        </div>
      </div>
    </nav>
  </div>
</header>

 

      


<main class="container">
  <div class="row justify-content-between">
    <div class="col-lg-12" id="content_column">
      <article class="article">

        <a class="h1 d-block mb-3" href="/blog">DeclareDesign Blog</a>
        <h2 class="article-title">What can you learn from simulating qualitative inference strategies?</h2>

        
        <span class="article-date">2019/01/30</span>
        

        <div class="article-content">
          
<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>


<p>Qualitative process-tracing sometimes seeks to answer “cause of effects” claims using within-case data: how probable is the hypothesis that <span class="math inline">\(X\)</span> <em>did in fact</em> cause <span class="math inline">\(Y\)</span>? <span class="citation">Fairfield and Charman (2017)</span>, for example, ask whether the right changed position on tax reform during the 2005 Chilean presidential election (<span class="math inline">\(Y\)</span>) <em>because</em> of anti-inequality campaigns (<span class="math inline">\(X\)</span>) by examining whether the case study narrative bears evidence that you would only expect to see if this were true.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> When inferential logics are so clearly articulated, it becomes possible to do design declaration and diagnosis. Here we declare a Bayesian process-tracing design and use it to think through choices about what kinds of within-case information have the greatest probative value.</p>
<p>Say we want to evaluate a case-specific hypothesis, <span class="math inline">\(H\)</span>, regarding whether <span class="math inline">\(Y\)</span> happened <em>because</em> <span class="math inline">\(X\)</span> happened. The hypothesis is not that <span class="math inline">\(X\)</span> is the only cause of <span class="math inline">\(Y\)</span>, but more simply whether <span class="math inline">\(Y\)</span> would have been different had <span class="math inline">\(X\)</span> been different. A researcher looks for “clues” or evidence, <span class="math inline">\(E\)</span>, in a case narrative or other qualitative data, which would be more or less surprising to see depending on whether <span class="math inline">\(H\)</span> is true. <span class="citation">Collier (2011)</span> lays out the basic strategy. In a recent paper, <span class="citation">Murtas, Dawid, and Musio (2017)</span> show how to justify updating case level inferences from experimental data on moderators and mediators.</p>
<p>Formally declaring and diagnosing such a procedure yields two non-obvious insights:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Straws in the wind can be stronger than smoking guns</strong>: Strong but rare clues do not always give you better answers <em>on average</em> than weak but common clues.</p></li>
<li><p><strong>The joint distribution of clues can be really important</strong>: Many applications of Bayesian process-tracing implicitly assume that clues are generated independently. Yet, when clues are negatively correlated (i.e., they arise through <em>alternative</em> causal pathways), they are jointly much more informative than when they are positively correlated.</p></li>
</ol>
<div id="declaring-a-process-tracing-design" class="section level1">
<h1>Declaring a process-tracing design</h1>
<p>There are different approaches to process-tracing. We focus here on “theory testing” rather than exploratory process-tracing and use an approach that draws on the potential outcomes framework (see for example <span class="citation">Humphreys and Jacobs (2015)</span>).</p>
<p>We consider a simple example: you choose one country in which there was a civil war (<span class="math inline">\(Y\)</span>) and natural resources (<span class="math inline">\(X\)</span>), and look for evidence (<span class="math inline">\(E\)</span>) that helps you update beliefs about <span class="math inline">\(Pr(H)\)</span>—the probability that the civil war happened <em>because</em> natural resources were present (H/T <span class="citation">Ross (2004)</span>).</p>
<div id="model-inquiry-data" class="section level2">
<h2>Model-Inquiry-Data</h2>
<p>If we think of causal relations in counterfactual terms there are just four possible causal relationships between a binary <span class="math inline">\(X\)</span> and a binary <span class="math inline">\(Y\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p>The presence of natural resources could cause civil war (<span class="math inline">\(X\)</span> causes <span class="math inline">\(Y\)</span>).</p></li>
<li><p>The presence of natural resources could be the only thing <em>preventing</em> war (<span class="math inline">\(\neg X\)</span> causes <span class="math inline">\(Y\)</span>).</p></li>
<li><p>Civil war might happen irrespective of whether natural resources are present (<span class="math inline">\(Y\)</span> irrespective of <span class="math inline">\(X\)</span>).</p></li>
<li><p>Civil war might not happen irrespective of whether natural resources are present (<span class="math inline">\(\neg Y\)</span> irrespective of <span class="math inline">\(X\)</span>).</p></li>
</ol>
<p>For the simulations, we will imagine we are in a world with 195 countries of which roughly 30% have natural resources (<span class="math inline">\(X\)</span>) (that’s easy to specify). We will also specify a model in which civil war is governed by causal pathway 1 (<span class="math inline">\(X\)</span> causes <span class="math inline">\(Y\)</span>) in roughly 20% of countries, by pathway 2 (<span class="math inline">\(\neg X\)</span> causes <span class="math inline">\(Y\)</span>) in only 10% of countries, by pathway 3 (<span class="math inline">\(Y\)</span> irrespective of <span class="math inline">\(X\)</span>) in 20% of countries, and by pathway 4 (<span class="math inline">\(\neg Y\)</span> irrespective of <span class="math inline">\(X\)</span>) in half of all countries (that’s not so easy to specify and of course is information that is not available at the answer stage).</p>
<p>In addition, we imagine that there is further “process” data that is informative about causal relations. We imagine two types (see <span class="citation">Collier (2011)</span> for a discussion of clues of this type):</p>
<ul>
<li><p><strong>A straw-in-the-wind clue.</strong> A straw-in-the-wind clue is an outcome that is somewhat more likely to be present if the hypothesized causal process is in operation and somewhat less likely if it is not. Let’s say, for example, that <span class="math inline">\(E_1\)</span> is the national army taking control over natural resources during a civil war. We imagine that that’s likely to happen if the natural resources caused the war: <span class="math inline">\(Pr(E_1 \mid H) = .75\)</span>. But even if the natural resources didn’t cause the war, the national army might still take over natural resources for other reasons, say <span class="math inline">\(Pr(E_1 \mid \neg H) = .25\)</span>.</p></li>
<li><p><strong>A smoking gun clue</strong>. A smoking gun clue is an outcome that is somewhat likely to be present if the stipulated hypothesis is true, but very unlikely if it is false. Say one of the antagonists was an armed group whose main name, aims, and ideology were centered around the capture and control of natural resources. This information provides a clue which might be really unlikely to arise in general, even if <span class="math inline">\(H\)</span> is true. But it’s very informative if it is observed, since it’s so unlikely to arise if <span class="math inline">\(H\)</span> is not true: it’s a “smoking gun.” Let’s say <span class="math inline">\(Pr(E_2 \mid H) = .3, Pr(E_2 \mid \neg H) = 0.05\)</span>.</p></li>
</ul>
<p>These clues might themselves be mediators, or moderators, or even arise post treatment, though we do not specify the full causal model that gives rise to them here. Rather, we simply define a step that generates these clue observations independently, conditional on the causal process. This is a strong assumption: the fact that an armed group formed in order to take resources (<span class="math inline">\(E_2\)</span>) might convince the government to take over the natural resource (<span class="math inline">\(E_1\)</span>) – or it might dissuade the government! We therefore relax this “Independent Clues” assumption below.</p>
<p>This gives us enough information to put down the stub of a design in which a model generates data with these features, an imaginary researcher samples one case from the <span class="math inline">\(X=Y=1\)</span> group, and defines the question the researcher wants to ask about this case. Notice here the inquiry takes place after the sampling because we care about what happens in the specific case we chose.</p>
<pre class="r"><code>design_stub &lt;- 
    
  declare_population(
      N = 195, 
      X = rbinom(N, 1, .3) == 1,
      causal_process = sample(c(&#39;X_causes_Y&#39;, &#39;X_causes_not_Y&#39;, &#39;Y_regardless&#39;, &#39;not_Y_regardless&#39;), 
                              N, replace = TRUE, prob = c(.2, .1, .2, .5)),
      Y = (X &amp; causal_process == &quot;X_causes_Y&quot;) |     
          (!X &amp; causal_process == &quot;X_causes_not_Y&quot;) |
          (causal_process == &quot;Y_regardless&quot;))  +
  
  declare_sampling(strata = (X == 1 &amp; Y == 1), strata_n = c(&quot;FALSE&quot; = 0, &quot;TRUE&quot; = 1)) +
  
  declare_step(
    SIW_observed = rbinom(
      n = N, size = 1, prob = ifelse(test = causal_process == &#39;X_causes_Y&#39;, .75, .25)),
    SMG_observed = rbinom(
      n = N, size = 1, prob = ifelse(test = causal_process == &#39;X_causes_Y&#39;, .3,  .05)),
    handler = fabricate, label = &quot;Independent Clues&quot;) +
  
    
  declare_estimand(did_X_cause_Y = causal_process == &#39;X_causes_Y&#39;) </code></pre>
<p>So far, a dataset from this design stub might look like this:</p>
<pre class="r"><code>draw_data(design_stub) %&gt;% kable(digits = 2, align = &quot;c&quot;)</code></pre>
<table>
<thead>
<tr class="header">
<th align="center">ID</th>
<th align="center">X</th>
<th align="center">causal_process</th>
<th align="center">Y</th>
<th align="center">S_inclusion_prob</th>
<th align="center">fab_ID_1</th>
<th align="center">SIW_observed</th>
<th align="center">SMG_observed</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">055</td>
<td align="center">TRUE</td>
<td align="center">X_causes_Y</td>
<td align="center">TRUE</td>
<td align="center">0.05</td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">0</td>
</tr>
</tbody>
</table>
</div>
<div id="answer-strategy" class="section level2">
<h2>Answer strategy</h2>
<p>We now turn to the answer strategy. For this, we’ll assume that at the analysis stage researchers use Bayes’ rule to figure out <span class="math inline">\(Pr(H \mid E)\)</span>: the posterior probability that <span class="math inline">\(X\)</span> caused <span class="math inline">\(Y\)</span> in the case we chose, given the clue evidence we found. We make a function that calculates the posterior using Bayes’ rule:</p>
<p><span class="math display">\[Pr(H \mid E) = \frac{Pr(H) Pr(E|H)}{Pr(H)Pr(E\mid H) + Pr(\neg H)Pr(E\mid\neg H)}\]</span></p>
<pre class="r"><code>calculate_posterior &lt;- function(data, p_H, p_clue_found_H, p_clue_found_not_H, test, label) {
  clue_found &lt;- data[, test]
  p_E_H &lt;- ifelse(clue_found, p_clue_found_H, 1 - p_clue_found_H)
  p_E_not_H &lt;- ifelse(clue_found, p_clue_found_not_H, 1 - p_clue_found_not_H)
  data.frame(posterior_H = p_E_H * p_H / (p_E_H * p_H + p_E_not_H * (1 - p_H)), clue_found = clue_found)}</code></pre>
<p>Bayes’ rule makes use of the probability of observing <span class="math inline">\(E\)</span> if <span class="math inline">\(H\)</span> is true and the probability of observing <span class="math inline">\(E\)</span> if <span class="math inline">\(H\)</span> is not true. The more different these probabilities are the more you learn from new data.</p>
<p>We also need to specify the imaginary researcher’s prior belief that <span class="math inline">\(H\)</span> is true. The imaginary researcher knows that only two processes, 1 and 3 from above, could have generated the data <span class="math inline">\(X = Y = 1\)</span>. Thus, they might specify a “flat” prior: <span class="math inline">\(Pr(H) = .5\)</span> (though they might have more informed beliefs from background knowledge).</p>
<p>We use the <code>calculate_posterior()</code> function we made above to declare two different answer strategies: one predicated on the straw-in-the-wind, and the other on the smoking gun.</p>
<pre class="r"><code>design &lt;-
  
  design_stub + 
  
  declare_estimator(
    test               = &quot;SIW_observed&quot;, 
    p_H                = .5, 
    p_clue_found_H     = .75,
    p_clue_found_not_H = .25,
    label              = &quot;Straw in Wind&quot;,
    estimand           = &quot;did_X_cause_Y&quot;,
    handler            = label_estimator(calculate_posterior)) +
  
  declare_estimator(
    test               = &quot;SMG_observed&quot;, 
    p_H                = .5, 
    p_clue_found_H     = .30,
    p_clue_found_not_H = .05,
    label              = &quot;Smoking gun&quot;,
    estimand           = &quot;did_X_cause_Y&quot;,
    handler            = label_estimator(calculate_posterior)) </code></pre>
</div>
</div>
<div id="diagnosis" class="section level1">
<h1>Diagnosis</h1>
<p>With this declaration, we can use <code>simulate_design(design)</code> to simulate the design many times, and then see how the strategies perform on average.</p>
<p>Below we plot the distribution of inferences, <span class="math inline">\(Pr(H|E)\)</span>, given strategies and true causal processes. The dotted lines show the true values of <span class="math inline">\(Pr(H)\)</span>. The left column represents guesses when conditioning on the straw-in-the-wind, whereas the right column represents guesses when conditioning on the smoking gun. In both cases, the results are “unbiased” (yes we can assess bias from diagnosis even for a Bayesian design)—because it so happened here that the distribution of causal pathways specified for the background model <em>and</em> the true probabilities of the clues matched the those used in the researcher’s answer strategy.</p>
<pre><code>## `summarise()` ungrouping output (override with `.groups` argument)</code></pre>
<p><img src="/blog/2019-01-30-process-tracing_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>As expected, the smoking gun can be highly probative: the bottom right panel shows that we sometimes get the answer exactly right (i.e., in those cases when <span class="math inline">\(H\)</span> is true and we observe a smoking gun). Note, however, that this is pretty rare: most of the time we guess <span class="math inline">\(Pr(H\mid E_2) =\)</span> 0.25, which is quite far from the true values of 0 and 1. We sometimes, but very rarely, get a false smoking gun. The smoking gun distribution is one that is centered close to the prior but sometimes makes big, and usually correct, inferential leaps.</p>
<p>By contrast, the left column shows that the straw-in-the-wind strategy gets pretty close pretty often. Whenever it is observed, the researcher guesses <span class="math inline">\(Pr(H\mid E_1) =\)</span> 0.86, and when it is not they guess <span class="math inline">\(Pr(H\mid E_1) =\)</span> 0.42. This means they sometimes make mistakes – because they observe <span class="math inline">\(E_1\)</span> even when <span class="math inline">\(H\)</span> is false, and vice versa. But it happens rarely enough that they tend to move in the right direction most of the time.</p>
<p>The net result is that, in this case, the straw-in-the-wind test has lower root mean squared error (RMSE): 0.47 vs. the smoking gun’s 0.43. It’s less wrong more often. In general, however, which type of clue provides better inferences depends on the particular probabilities assumed.</p>
</div>
<div id="using-more-than-one-clue" class="section level1">
<h1>Using more than one clue</h1>
<p>In practice, researchers seldom pre-commit to updating from a single piece of evidence, but search for multiple clues and update about their cause of effects hypotheses jointly. As mentioned above, however, doing so in fact requires that we specify the joint distribution of the clues.</p>
<p>In the code appendix below, we create a function (<code>joint_prob()</code>) for specifying the joint distribution of the clues, given their marginal probabilities and the correlation between them (<code>rho</code>). We also replace the “Independent Clues” step with a “Correlated Clues” step, in which clues arise according to this joint distribution. This enables us to add a strategy in which we update in light of both clues simultaneously.</p>
<p>We skip that code here, and simply show how the RMSE changes as a function of <code>rho</code> for the three different strategies.</p>
<pre><code>## `summarise()` regrouping output by &#39;rho_H&#39; (override with `.groups` argument)</code></pre>
<p><img src="/blog/2019-01-30-process-tracing_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>As expected, using more information (both of the clues) gets you much better answers on average. However, notice that the gains from a joint approach are much greater when the clues are negatively correlated than when they are positively correlated.</p>
<p>This feature arises because the pieces of evidence carry less independent information when they are positively correlated. To see this, suppose they were perfectly correlated, so that observing one clue guarantees that you would observe the other. In this case, there is no additional information gleaned from the observation of one clue once the other has been observed: they are effectively equivalent tests.
So, one implication is that process-tracers may do better by looking for clues that are negatively correlated. What might this look like in practice?</p>
<p>Negatively correlated clues might arise if they result from processes that substitute for each other. For example, if the national army is less likely to take control of the natural resources precisely when an armed group has declared that it will fight for them. Positively correlated clues might arise if they result from common processes. For example, if the national army takes control over natural resources precisely because this counters the stated strategic objectives of the armed group.</p>
</div>
<div id="takeaways" class="section level1">
<h1>Takeaways</h1>
<p>We see four main takeaways here:</p>
<ol style="list-style-type: decimal">
<li><p>The frequency of clues matters: against much of the advice in the process-tracing literature,<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> a researcher who had to choose one clue to make an inference would do better to choose a straw-in-the-wind than a smoking gun.</p></li>
<li><p>The joint distribution of clues matters: very often we would expect clues to arise through interrelated causal processes. Researchers can in fact make more powerful inferences if they can defend the assumption that clues are negatively correlated.</p></li>
<li><p>Declaring and diagnosing designs forces you to think them through in a way that can make non-obvious features and assumptions obvious. At a minimum, formalizing your design in this manner can highlight what kinds of beliefs are necessary to justify inferences.</p></li>
<li><p>With that said, we also see here a lesson on the limits of declaration. Even with just two clues, declaring a process-tracing inferential strategy makes serious demands on researcher knowledge. In particular, the ability to make claims about the <em>joint</em> distribution of clues under alternative causal hypotheses. In principle, it is possible to base such beliefs on external data (see <span class="citation">Murtas, Dawid, and Musio (2017)</span>) but doing so is hard and the challenges rise exponentially as more and more complex clues are considered. The declaration strategy works best, perhaps, for major claims resulting from key evidence. It seems unlikely to be able to capture inferences made from all the complex evidence used by process tracers.</p></li>
</ol>
</div>
<div id="code-appendix" class="section level1">
<h1>Code appendix</h1>
<pre class="r"><code># Calculate bivariate probabilities given correlation
joint_prob &lt;- function(p1, p2, rho) {
  r &lt;- rho * (p1 * p2 * (1 - p1) * (1 - p2)) ^ .5
  c(`00` = (1 - p1) * (1 - p2) + r,
    `01` = p2 * (1 - p1) - r,
    `10` = p1 * (1 - p2) - r,
    `11` = p1 * p2 + r)}

rho_H &lt;- 0
rho_not_H &lt;- 0

calculate_posterior_joint &lt;- function(data, p_H, p_clue_1_found_H, p_clue_1_found_not_H, p_clue_2_found_H, p_clue_2_found_not_H, rho_H, rho_not_H, test){
  clue_found &lt;- data[, test]
  p_E_H &lt;- joint_prob(p1 = p_clue_1_found_H, p2 = p_clue_2_found_H, rho = rho_H)[clue_found]
  p_E_not_H &lt;- joint_prob(p1 = p_clue_1_found_not_H, p2 = p_clue_2_found_not_H, rho = rho_not_H)[clue_found]
  data.frame(posterior_H = p_E_H * p_H / (p_E_H * p_H + p_E_not_H * (1 - p_H)), clue_found = clue_found)
}
  
corr_design &lt;- replace_step(
  design = design, 
  step = &quot;Independent Clues&quot;,
  new_step =  declare_step(
    SIW_SMG = sample(c(&quot;00&quot;, &quot;01&quot;, &quot;10&quot;, &quot;11&quot;),1, 
                     prob = {
                       if(causal_process == &quot;X_causes_Y&quot;) 
                         joint_prob(.75, .30, rho_H) 
                       else 
                         joint_prob(.25,  .05, rho_not_H)
                     }),
  SIW_observed = SIW_SMG == &quot;10&quot; | SIW_SMG == &quot;11&quot;,
  SMG_observed = SIW_SMG == &quot;01&quot; | SIW_SMG == &quot;11&quot;,
  handler = fabricate,
  label = &quot;Correlated Clues&quot;))

corr_design &lt;- corr_design + 
  declare_estimator(
    test                 = &quot;SIW_SMG&quot;, 
    p_H                  = .5, 
    p_clue_1_found_H     = .75,
    p_clue_1_found_not_H = .25,
    p_clue_2_found_H     = .30,
    p_clue_2_found_not_H = .05,
    rho_H                = rho_H,
    rho_not_H            = rho_not_H,
    label                = &quot;Update from both clues&quot;,
    estimand             = &quot;did_X_cause_Y&quot;,
    handler              = label_estimator(calculate_posterior_joint)) </code></pre>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-collier2011understanding">
<p>Collier, David. 2011. “Understanding Process Tracing.” <em>PS: Political Science &amp; Politics</em> 44 (4): 823–30.</p>
</div>
<div id="ref-fairfield2017explicit">
<p>Fairfield, Tasha, and Andrew E. Charman. 2017. “Explicit Bayesian Analysis for Process Tracing: Guidelines, Opportunities, and Caveats.” <em>Political Analysis</em> 25 (3): 363–80.</p>
</div>
<div id="ref-humphreys2015mixing">
<p>Humphreys, Macartan, and Alan M Jacobs. 2015. “Mixing Methods: A Bayesian Approach.” <em>American Political Science Review</em> 109 (4): 653–73.</p>
</div>
<div id="ref-murtas2017new">
<p>Murtas, Rossella, Alexander Philip Dawid, and Monica Musio. 2017. “New Bounds for the Probability of Causation in Mediation Analysis.” <em>arXiv Preprint arXiv:1706.04857</em>.</p>
</div>
<div id="ref-ross2004natural">
<p>Ross, Michael L. 2004. “How Do Natural Resources Influence Civil War? Evidence from Thirteen Cases.” <em>International Organization</em> 58 (1): 35–67.</p>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>For example: “the former President [said] that the tax subsidy ‘never would have been eliminated if I had not taken [the opposition candidate] at his word’ when the latter publicly professed concern over inequality.”<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>E.g. <span class="citation">Collier (2011)</span>: of the four process-tracing tests, straws-in-the-wind are ``the weakest and place the least demand on the researcher’s knowledge and assumptions.’’ (826)<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
</ol>
</div>

        </div>
      </article>
      



    </div>
  </div>
</main>

    </div>

    

<footer>
    <div class="footer-top">
      <div class="container">
        <div class="row no-gutters">
          <div class="col-lg-7 col-md-12">
            <div class="row">
              <div class="col-md-3 col-12">
                <div class="footer-brand"><a href=""><img src="/images/brand-footer.svg" alt=""></a></div>
              </div>
              <div class="col-md-3 col-4">
                <p>DeclareDesign</p>
                <ul class="list-unstyled">
                  <li><a href="/">Home</a></li>
                  <li><a href="/about.html">About</a></li>
                  <li><a href="/r/designlibrary">Library</a></li>
                  <li><a href="/blog.html">Blog</a></li>
                  <li><a href="http://discuss.declaredesign.org/">Help</a></li>
                </ul>
              </div>
              <div class="col-md-3 col-4">
                <p>Software</p>
                <ul class="list-unstyled">
                  <li><a href="/r/declaredesign/">DeclareDesign</a></li>
                  <li><a href="/r/estimatr/">estimatr</a></li>
                  <li><a href="/r/randomizr/">randomizr</a></li>
                  <li><a href="/r/fabricatr/">fabricatr</a></li>
                  <li><a href="/r/designlibrary/">DesignLibrary</a></li>
                </ul>
              </div>
              
            </div>
          </div>
          <div class="col-lg-5 col-md-12"></div>
        </div>
      </div>
    </div>
    <div class="footer-bottom">
      <div class="footer-meta-block">
        <div class="container">
          <div class="row align-items-center">
            <div class="col-sm-7">
              <p class="mb-0">&copy; 2021 Graeme Blair, Jasper Cooper, Alexander Coppock, and Macartan Humphreys</p>
            </div>
            <div class="col-sm-5">
              <div class="fbl-link-wrap">
                <ul class="list-inline mb-0">
                  <li class="list-inline-item"> <a target="_blank" href="http://github.com/DeclareDesign/"><img src="images/icon-git.svg" alt=""></a> </li>
                </ul>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </footer>
  
   
  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy" crossorigin="anonymous"></script>

    <link rel="stylesheet" href="https://cdn.datatables.net/v/bs4/dt-1.10.18/datatables.min.css"/>
<script src="https://cdn.datatables.net/v/bs4/dt-1.10.18/datatables.min.js"></script>

<script>
    jQuery(function ()
    {
        const library_list = jQuery("#design_library_list");
        library_list.addClass("table table-striped table-bordered");
        library_list.DataTable(
            {
                "autoWidth": false,
                "columns":
                    [
                        {"width": "30%"}, 
                        {"width": "10%"}, 
                        {"width": "10%"}, 
                        {"width": "10%"}, 
                        {"width": "15%"}, 
                        {"width": "25%"}, 
                    ],
                "drawCallback": initialize_tooltips
            }
        );

        library_list.css("width", "");
    });

    function initialize_tooltips()
    {
        const tooltip_elements = jQuery('[data-toggle="tooltip"]');
        tooltip_elements.tooltip();
        tooltip_elements.click(function ()
        {
            jQuery(this).tooltip("hide");
        });
    }
</script>

    


<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>
jQuery(function ()
{
  $('pre').each(function (index)
  {
    hljs.highlightBlock(this);
  });
});
</script>



    
<script src="/js/math-code.js"></script>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    



    
  </body>
</html>

